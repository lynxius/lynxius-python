{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MyMedScribe - A Performant and Reliable AI Medical Scribe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook shows how to evaluate an LLM App unsing \n",
    "[Lynxius](https://www.lynxius.ai/) SDK.\n",
    "\n",
    "The application being under examination is **MyMedScribe**, an AI Medical Scribe that \n",
    "assists medical doctors by capturing notes during patient visits. The App workflow \n",
    "follows:\n",
    "\n",
    "1. üé§ Notes are recorded with the the doctor's smartphone üéôÔ∏è\n",
    "2. üß† **MyMedScribe**'s cleaver AI filters out small talk, summarizes content, identifies \n",
    "    key medical information and generates clinical documentation ü§ñ\n",
    "3. ü©∫ The doctor reviews the AI-generated notes and signs off ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Patient Consultation\n",
    "\n",
    "* **Brenda Hills** experiences chest pain and seeks medical advice from her physician. \n",
    "* Her physician, **Dr. Sara Taylor**, utilizes the **MyMedScribe** App to streamline her \n",
    "  documentation process.\n",
    "\n",
    "For Dr. Taylor, it is imperative that the transcripts and clinical notes produced by \n",
    "**MyMedScribe** are of exceptional quality: they need to be accurate, comprehensive, and \n",
    "free from any errors, ensuring no vital information is omitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .adaptive-text { background-color: #fffb00; } /* Light text in dark theme */\n",
    "    }\n",
    "    @media (prefers-color-scheme: light) {\n",
    "        .adaptive-text { background-color: #fffb00; } /* Dark text in light theme */\n",
    "    }\n",
    "</style>\n",
    "\n",
    "## Jupyter Notebook Setup\n",
    "\n",
    "This Jupyter notebook requires a small setup to start. We need to import the relevant \n",
    "packages and utility functions. To consume [Lynxius Platform](https://platform.lynxius.ai/) \n",
    "we need to provide `LYNXIUS_API_KEY` secret key.\n",
    "Today Lynxius offers OpenAI GPT-4 by default as LLM model for \n",
    "evaluation, but we are building our own models for more accurate evaluations.\n",
    "\n",
    "‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è <span class=\"adaptive-text\">At the moment Lynxius's OpenAI key is used, but we plan \n",
    "to enable also users to use their own keys or directly use our proprietary models. \n",
    "</span> ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "\n",
    "from AI_medical_scribe_utils import load_input, load_yaml_input\n",
    "\n",
    "# Making sure a local version of the Lynxius library is available\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "\n",
    "# set Lynxius secret key\n",
    "if not (lynxius_api_key := os.getenv(\"LYNXIUS_API_KEY\")):\n",
    "    lynxius_api_key = getpass(\"üîë Enter your Lynxius API key: \")\n",
    "\n",
    "os.environ[\"LYNXIUS_API_KEY\"] = lynxius_api_key\n",
    "\n",
    "os.environ[\"LYNXIUS_BASE_URL\"] = \"http://REQUEST-US-WHICH-ENDOINT-TO-USE-TO-GET-ACCESS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-1: Evaluating Summarization Quality\n",
    "\n",
    "**MyMedScribe** App filters and summarizes the patient visit and generates a clean \n",
    "transcript.\n",
    "\n",
    "<img src=\"https://github-public-assets.s3.us-west-1.amazonaws.com/AI_medical_scribe_sumarization.png\" alt=\"AI Medical Scribe summarization\" width=\"60%\" />\n",
    "\n",
    "‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è **The AI-generated summarization looks good, but... How good is it?** ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\n",
    "\n",
    "**MyMedScribe** team knows how crucial it is to Dr. Taylor that the AI-generated \n",
    "transcript contains all the relevant information of the original conversation with \n",
    "Brenda. **MyMedScribe** team must evaluate their product before and after deploying it to \n",
    "production and chooses [Lynxius](https://www.lynxius.ai/) for testing and evaluating \n",
    "their LLM App."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Information Score\n",
    "\n",
    "**MyMedScribe** team wants to evaluate the summarization performance of their LLM App. \n",
    "They care to know if the AI-generated transcript is semantically similar to the original \n",
    "raw transcript recorded by Dr. Taylor's smartphone and if it missing any relevant \n",
    "information.\n",
    "\n",
    "[Lynxius](https://www.lynxius.ai/) `BertScore` evaluator is perfect as the \n",
    "**\"Missing Information Score\"**. It takes the original raw transcript and the \n",
    "AI-generated summary as inputs and determines their semanthic similarity.\n",
    "\n",
    "<img src=\"https://github-public-assets.s3.us-west-1.amazonaws.com/AI_medical_scribe_eval_sumarization.png\" alt=\"AI Medical Scribe eval summarization\" width=\"60%\" />\n",
    "\n",
    "Let's see the `BertScore` evaluator in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw transcript and AI-generated summary\n",
    "raw_transcript = load_input('./data/AI_medical_scribe_raw.yaml')\n",
    "ai_summary = load_input('./data/AI_medical_scribe_summary.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: {\"uuid\":\"b34371a3-5074-433c-ac74-7615f0f9fa68\"}\n"
     ]
    }
   ],
   "source": [
    "from lynxius import Lynxius\n",
    "\n",
    "client = Lynxius()\n",
    "\n",
    "# Calculate the BertScore and upload results to Lynxius Platform\n",
    "client.evaluate(\n",
    "    title=\"Bert Score v1\",\n",
    "    evaluator=\"bert_score\",\n",
    "    reference=raw_transcript,\n",
    "    output=ai_summary,\n",
    "    level=\"sentence\",\n",
    "    presence_threshold=0.65,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously set our Project's Bert Score Thresholds to the following values:\n",
    "\n",
    "<table>\n",
    "  <thead style=\"background-color: #919191;\">\n",
    "    <tr>\n",
    "      <th>Metric</th>\n",
    "      <th>Value</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Precision</td>\n",
    "      <td>0.75</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Recall</td>\n",
    "      <td>0.55</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>F1</td>\n",
    "      <td>0.60</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "Below a screenshot of the Bert Score results from [Lynxius Platform](platform.lynxius.ai).\n",
    "\n",
    "<img src=\"https://github-public-assets.s3.us-west-1.amazonaws.com/AI_medical_scribe_BertScore_v1.png\" alt=\"AI Medical Scribe BertScore v1\" width=\"90%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùå‚ùå‚ùå **Ouch!!! something went wrong** ‚ùå‚ùå‚ùå\n",
    "\n",
    "Looks like `BERTScore Precision` and `BERTScore Recall` are lower than the \n",
    "set threshold. Thankfully `BERTScore` also produce a list of tokens (sentences in our \n",
    "case) that are not present in the summary and [Lynxius Platform](platform.lynxius.ai) \n",
    "displays it nicely on the side pannel view:\n",
    "\n",
    "<img src=\"https://github-public-assets.s3.us-west-1.amazonaws.com/AI_medical_scribe_BertScore_v1_missing_info.png\" alt=\"AI Medical Scribe BertScore v2\" width=\"90%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .adaptive-text { background-color: #fc4949; } /* Light text in dark theme */\n",
    "    }\n",
    "    @media (prefers-color-scheme: light) {\n",
    "        .adaptive-text { background-color: #fcbdbd; } /* Dark text in light theme */\n",
    "    }\n",
    "</style>\n",
    "\n",
    "‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è **Looks like some information was lost during AI-generated summarization** ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\n",
    "\n",
    "The phrase <span class=\"adaptive-text\">We'll also keep you here for a bit for \n",
    "observation. We're planning to do an X-ray and an ECG to check your heart and lungs more \n",
    "closely. This will help us understand what's causing your chest pain.</span> is pretty \n",
    "critical, but isn't in the AI-generated summary.\n",
    "\n",
    "**MyMedScribe** uses the insights coming from [Lynxius](https://www.lynxius.ai/) to further \n",
    "debug and improve their product. After a few days of working **MyMedScribe** team realizes \n",
    "that there was a problem with the way they were chunking the script sentences. After \n",
    "fixing the issue, they are ready to test again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the new AI-generated summary\n",
    "new_ai_summary = load_input('./data/AI_medical_scribe_new_summary.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: {\"uuid\":\"c8db892c-991c-4bd4-930c-c64280bdc851\"}\n"
     ]
    }
   ],
   "source": [
    "# Calculate the BertScore and upload results to Lynxius Platform\n",
    "client.evaluate(\n",
    "    title=\"Bert Score v2\",\n",
    "    evaluator=\"bert_score\",\n",
    "    reference=raw_transcript,\n",
    "    output=new_ai_summary,\n",
    "    level=\"sentence\",\n",
    "    presence_threshold=0.65,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remember the values of the Bert Score Thresholds set for this project:\n",
    "\n",
    "<table>\n",
    "  <thead style=\"background-color: #919191;\">\n",
    "    <tr>\n",
    "      <th>Metric</th>\n",
    "      <th>Value</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Precision</td>\n",
    "      <td>0.75</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Recall</td>\n",
    "      <td>0.55</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>F1</td>\n",
    "      <td>0.60</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "Below a screenshot of the new Bert Score results from [Lynxius Platform](platform.lynxius.ai).\n",
    "\n",
    "<img src=\"https://github-public-assets.s3.us-west-1.amazonaws.com/AI_medical_scribe_BertScore_v2.png\" alt=\"AI Medical Scribe BertScore v2\" width=\"90%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâüéâüéâ **Hurray!!!** üéâüéâüéâ\n",
    "\n",
    "The AI-generated summary is finally accurate ‚úÖ‚úÖ‚úÖ. **MyMedScribe** team now wants \n",
    "to evaluate now their App's performance with filing a clinical documentation report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-2: Evaluating Writing Quality\n",
    "\n",
    "**MyMedScribe** App uses the new AI-generated summary to generate the clinical \n",
    "documentation report.\n",
    "\n",
    "<img src=\"https://github-public-assets.s3.us-west-1.amazonaws.com/AI_medical_scribe_clinical_notes_with_error.png\" alt=\"AI Medical Scribe clinical notes with error\" width=\"60%\" />\n",
    "\n",
    "‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è **The AI-generated clinical documentation looks good, but... How good is it?** ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\n",
    "\n",
    "**MyMedScribe** team knows that Dr. Taylor has to file the clinical documentation \n",
    "electronically and share the report with other doctors. The helath of Brenda, as well as \n",
    "Dr. Taylor's reputation are at stake. The report must respect all the typical writing \n",
    "quality guidelines and must contain all relevant information at the right place. \n",
    "**MyMedScribe** team must evaluate their product before and after deploying it to \n",
    "production and chooses [Lynxius](https://www.lynxius.ai/) for testing and evaluating \n",
    "their LLM App."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contains Medication Prescription Evaluator\n",
    "\n",
    "**MyMedScribe** team wants to make sure that AI-generated  the clinical documentation \n",
    "report always contains a drug prescription and that this is always written in a specific \n",
    "order: medication, dosage, frequency.\n",
    "\n",
    "[Lynxius](https://www.lynxius.ai/) provides an easy interface to build use-case specific \n",
    "**custom evaluators**. Custom evaluators can be:\n",
    "* Heuristics\n",
    "* AI-assisted\n",
    "\n",
    "In our case, **MyMedScribe** team needs an AI-assisted evaluator. To implement one, they \n",
    "only need to create a new prompt that specifies the use-case dependent policy to respect.\n",
    "\n",
    "<img src=\"https://github-public-assets.s3.us-west-1.amazonaws.com/AI_medical_scribe_eval_clinical_notes.png\" alt=\"AI Medical Scribe eval clinical notes\" width=\"60%\" />\n",
    "\n",
    "Let's build the `MedicationPrescription` evaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the evaluator prompt template\n",
    "MEDICATION_PRESCRIPTION_BASE_TEMPLATE = \"\"\"\n",
    "You are given a clinical documentation report as output text and a list of \n",
    "medications. You must determine whether the given text report contains one medication \n",
    "prescription. The medication prescription must always be written in this order: \n",
    "medication, dosage and frequency. The medication must belong to the list of medications \n",
    "received as input list. Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Output]: {output}\n",
    "    ************\n",
    "    [List]: {list}\n",
    "    [END DATA]\n",
    "Your response must be composed of a score and an an explanation. Here is the output \n",
    "format:\n",
    "    [BEGIN OUTPUT]\n",
    "    **************\n",
    "    [Score]: score here\n",
    "    **************\n",
    "    [Explanation]: explanation here\n",
    "    [END OUTPUT]\n",
    "Your score must be a single word, either \"correct\" or \"incorrect\", and should not \n",
    "contain any text or characters aside from that word.\n",
    "\"correct\" means that the output text contains one medication prescription and the \n",
    "medication prescription is written in the order: medication, dosage and frequency, where \n",
    "the medication belongs to the input list of medications.\n",
    "\"incorrect\" means that the output text doesn not contains a medication prescription, \n",
    "or this is written in a different order, or the medication does not belong to the input \n",
    "list.\n",
    "Your explanation must be a short, step by step description of why you asigned this score \n",
    "to be sure your conclusion is correct. Avoid simply stating the correct answers at the \n",
    "outset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we created the `MedicationPrescription` evaluator, let's use it to assess the \n",
    "writing quality of **MyMedScribe** App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open AI-generated clinical documentation report and generate reference\n",
    "ai_clinical_docs = load_yaml_input('./data/AI_medical_scribe_clinical_notes.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: {\"uuid\":\"294ef63c-848b-457e-842c-eadaad510b8f\"}\n"
     ]
    }
   ],
   "source": [
    "# Calculate the BertScore and upload results to Lynxius Platform\n",
    "client.evaluate(\n",
    "    title=\"MedicationPrescription v1\",\n",
    "    evaluator=\"custom_eval\",\n",
    "    output=ai_clinical_docs,\n",
    "    str_list=\"[Atorvastatin, Metformin, Amoxicillin, Novasc, Albuterol]\",\n",
    "    prompt_template=MEDICATION_PRESCRIPTION_BASE_TEMPLATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below a screenshot of the MedicationPrescription results from [Lynxius Platform](platform.lynxius.ai).\n",
    "\n",
    "<img src=\"https://github-public-assets.s3.us-west-1.amazonaws.com/AI_medical_scribe_MedicationPrescription_v1.png\" alt=\"AI Medical Scribe MedicationPrescription v1\" width=\"90%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùå‚ùå‚ùå **Ouch!!! something went wrong** ‚ùå‚ùå‚ùå\n",
    "\n",
    "It seems like the `MedicationPrescription` is **INCORRECT**. Thankfully \n",
    "[Lynxius](https://www.lynxius.ai/) offers an explenation of the error. It seems like the \n",
    "prescription does not include the medication's frequency of use.\n",
    "\n",
    "<img src=\"https://github-public-assets.s3.us-west-1.amazonaws.com/AI_medical_scribe_clinical_notes_comparison.png\" alt=\"AI Medical Scribe clinical notes comparison\" width=\"60%\" />\n",
    "\n",
    "**MyMedScribe** team uses this insights to improve their product and after a few days of \n",
    "working, they fixed the issue and are ready to test again with the newly AI-generated \n",
    "clinical documentation report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open new AI-generated clinical documentation report and generate reference\n",
    "new_ai_clinical_docs = load_yaml_input('./data/AI_medical_scribe_new_clinical_notes.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: {\"uuid\":\"8787e806-e20c-4848-8619-700478a26a95\"}\n"
     ]
    }
   ],
   "source": [
    "# Calculate the BertScore and upload results to Lynxius Platform\n",
    "client.evaluate(\n",
    "    title=\"MedicationPrescription v2\",\n",
    "    evaluator=\"custom_eval\",\n",
    "    output=new_ai_clinical_docs,\n",
    "    str_list=\"[Atorvastatin, Metformin, Amoxicillin, Novasc, Albuterol]\",\n",
    "    prompt_template=MEDICATION_PRESCRIPTION_BASE_TEMPLATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below a screenshot of the new MedicationPrescription results from [Lynxius Platform](platform.lynxius.ai).\n",
    "\n",
    "<img src=\"https://github-public-assets.s3.us-west-1.amazonaws.com/AI_medical_scribe_MedicationPrescription_v2.png\" alt=\"AI Medical Scribe MedicationPrescription v2\" width=\"90%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâüéâüéâ Hurray!!! üéâüéâüéâ\n",
    "\n",
    "The clinical documentation report is finally accurate ‚úÖ‚úÖ‚úÖ. **MyMedScribe** team can \n",
    "finally deploy the newest version of their LLM App to production üõ†Ô∏èüõ†Ô∏èüõ†Ô∏è They can also \n",
    "keep using `BERTScore` and `MedicationPrescription` and many more \n",
    "[Lynxius](https://www.lynxius.ai/) evaluators to monitor the production behaviour of \n",
    "their product."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".lynxius-sdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
