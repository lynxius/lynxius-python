{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Evaluations Locally\n",
    "\n",
    "This notebook demonstrates how to perform evaluations locally on your machine and upload the results to the [Lynxius Platform](https://platform.lynxius.ai/auth/signup).\n",
    "\n",
    "With the local evaluation setup, you will need to manage the API keys for the models used in testing. Evaluation tasks will use your own compute resources and are blocking tasks. For a fully managed solution with non-blocking tasks, see [Run Evaluations Remotely](./eval_remotely.ipynb).\n",
    "\n",
    "To select the local evaluation setup, set `run_local=True` when creating the client. Example: `client = LynxiusClient(run_local=True)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to setup Lynxius API key\n",
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "if not (lynxius_api_key := os.getenv(\"LYNXIUS_API_KEY\")):\n",
    "    lynxius_api_key = getpass(\"ðŸ”‘ Enter your Lynxius API key: \")\n",
    "\n",
    "os.environ[\"LYNXIUS_API_KEY\"] = lynxius_api_key\n",
    "os.environ[\"LYNXIUS_BASE_URL\"] = \"https://platform.lynxius.ai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes it easier to iterate\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll be using OpenAI to evaluate locally so we have to set the API key\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lynxius.client import LynxiusClient\n",
    "\n",
    "client = LynxiusClient(run_local=True)\n",
    "\n",
    "# Download a dataset previously uploaded to the Lynxius Platform\n",
    "dataset_details = client.get_dataset_details(dataset_id=\"7eff0d38-50ee-4b5d-a30d-cf428288016c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our sample LLM application\n",
    "from datasets_utils import chatdoctor_v1\n",
    "\n",
    "# Importing the evaluators\n",
    "from lynxius.evals.bert_score import BertScore\n",
    "from lynxius.evals.answer_correctness import AnswerCorrectness\n",
    "from lynxius.evals.semantic_similarity import SemanticSimilarity\n",
    "from lynxius.evals.custom_eval import CustomEval\n",
    "from lynxius.evals.context_precision import ContextPrecision\n",
    "from lynxius.evals.json_diff import JsonDiff\n",
    "\n",
    "# ContextChunk represents a document retrieved from you RAG system\n",
    "from lynxius.rag.types import ContextChunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define sample RAG contexts.\n",
    "# Retrieval of context documents depends on the RAG database that you're using\n",
    "context = [\n",
    "    ContextChunk(document=\"Avoid close contact with people who are sick. When you are sick, keep your distance from others to protect them from getting sick, too.\", relevance=0.75),\n",
    "    ContextChunk(document=\"If possible, stay home from work, school, and errands when youâ€™re sick. You can go back to your normal activities when, for at least 24 hours, both are true:\", relevance=0.31)\n",
    "]\n",
    "\n",
    "# Define tags to make it easier to locate these eval runs on the Lynxius platform\n",
    "tags = [\"notebook\", \"experiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lynxius allows you to use your own evaluator templates\n",
    "# Let's define and use one!\n",
    "\n",
    "# When using a custom template, the only thing that you need to ensure is that\n",
    "# the final verdict is printed at the very bottom of the resonse, with no other characters.\n",
    "custom_eval_template = \"\"\"\n",
    "You are given a question, a reference answer and a candidate answer concerning a clinical matter.\n",
    "You must determine if the candidate answer covers exactly the same content as the reference answer.\n",
    "If the candidate answer contains additional information, or fails to mention something that is present\n",
    "in the reference answer, your verdict should be 'incorrect'. Otherwise, your verdict should be 'correct'.\n",
    "Provide a short explanation about how you arrived to your verdict. The verdict must be printed at the\n",
    "very bottom of your response, on a new line, and it must not contain any extra characters.\n",
    "Here is the data:\n",
    "***********\n",
    "Query: {query}\n",
    "***********\n",
    "Reference answer: {reference}\n",
    "***********\n",
    "Candidate answer: {output}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d4740065-2dee-408c-a250-1c947c55c953'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and run the evals\n",
    "bert_score = BertScore(\"PR #222\", level=\"word\", presence_threshold=0.65, tags=tags)\n",
    "answer_correctness = AnswerCorrectness(\"PR #222\", tags=tags)\n",
    "semantic_similarity = SemanticSimilarity(\"PR #222\", tags=tags)\n",
    "custom_eval = CustomEval(\"PR #222\", name=\"clinical_correctness\", prompt_template=custom_eval_template, tags=tags)\n",
    "context_precision = ContextPrecision(\"PR #222\", tags=tags)\n",
    "\n",
    "for entry in dataset_details.entries:\n",
    "    # Query your LLM\n",
    "    actual_output = chatdoctor_v1(entry.query)\n",
    "\n",
    "    # Add traces to the evals\n",
    "    bert_score.add_trace(reference=entry.reference, output=actual_output, context=context)\n",
    "    answer_correctness.add_trace(query=entry.query, reference=entry.reference, output=actual_output, context=context)\n",
    "    semantic_similarity.add_trace(reference=entry.reference, output=actual_output, context=context)\n",
    "    custom_eval.add_trace(values={\"query\": entry.query, \"reference\": entry.reference, \"output\": actual_output}, context=context)\n",
    "    context_precision.add_trace(query=entry.query, reference=entry.reference, context=context)\n",
    "\n",
    "# Run evals locally and store results in the Lynxius platform\n",
    "client.evaluate(bert_score)\n",
    "client.evaluate(answer_correctness)\n",
    "client.evaluate(semantic_similarity)\n",
    "client.evaluate(custom_eval)\n",
    "client.evaluate(context_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e9add6df-73b8-4b6a-ab85-d3b4ce3696e6'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and run the JsonDiff eval\n",
    "json_diff = JsonDiff(\"PR #222\", tags=tags)\n",
    "\n",
    "ref = {\n",
    "    \"prop1\": True,\n",
    "    \"prop2\": 0.85,\n",
    "    \"prop3\": [\n",
    "      {\n",
    "        \"prop4\": 0.92,\n",
    "        \"prop5\": 0.71\n",
    "      },\n",
    "      {\n",
    "        \"prop4\": 0.22,\n",
    "        \"prop5\": 1.0\n",
    "      }\n",
    "    ]\n",
    "}\n",
    "output = {\n",
    "    \"prop1\": False,\n",
    "    \"prop2\": 0.71,\n",
    "    \"prop3\": [\n",
    "      {\n",
    "        \"prop4\": 0.89,\n",
    "        \"prop5\": 0.55\n",
    "      },\n",
    "      {\n",
    "        \"prop4\": 0.34,\n",
    "        \"prop5\": 0.97\n",
    "      }\n",
    "    ]\n",
    "}\n",
    "# Weights is an optional parameter. \n",
    "# If not provided, each field will have an equal contribution to the overall score of every nested object.\n",
    "weights = {\n",
    "    \"prop1\": 0.5,\n",
    "    \"prop2\": 0.5,\n",
    "    \"prop3\": 1.0, # Default weights is 1.0 but we can also set it explicitly\n",
    "}\n",
    "\n",
    "json_diff.add_trace(reference=ref, output=output, weights=weights, context=context)\n",
    "client.evaluate(json_diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".lynxius-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
