{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8b360ed",
   "metadata": {},
   "source": [
    "# Run Evaluations Remotely\n",
    "\n",
    "This notebook demonstrates how to perform evaluations remotely using the [Lynxius Platform](https://platform.lynxius.ai/auth/signup).\n",
    "\n",
    "The remote evaluation setup is a fully managed service where the API keys for the models used in testing are provided and managed by the Lynxius team. Evaluation tasks are executed in the background, freeing you from delays and compute costs.\n",
    "\n",
    "To select the remote evaluation setup, set `run_local=False` when creating the client, or simply omit the parameter since the default value is `False`. Example: `client = LynxiusClient(run_local=False)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddd87eb4-a4ee-4485-ba10-048127563b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to setup Lynxius API key\n",
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "if not (lynxius_api_key := os.getenv(\"LYNXIUS_API_KEY\")):\n",
    "    lynxius_api_key = getpass(\"ðŸ”‘ Enter your Lynxius API key: \")\n",
    "\n",
    "os.environ[\"LYNXIUS_API_KEY\"] = lynxius_api_key\n",
    "os.environ[\"LYNXIUS_BASE_URL\"] = \"https://platform.lynxius.ai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61de21b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes it easier to iterate\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfddc746-4f0d-4889-be24-694a6bf618dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lynxius.client import LynxiusClient\n",
    "\n",
    "client = LynxiusClient()\n",
    "\n",
    "# Download a dataset previously uploaded to the Lynxius Platform\n",
    "dataset_details = client.get_dataset_details(dataset_id=\"7eff0d38-50ee-4b5d-a30d-cf428288016c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "934e9b78-38f4-4674-9817-92fa3e302f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our sample LLM application\n",
    "from datasets_utils import chatdoctor_v1\n",
    "\n",
    "# Importing the evaluators\n",
    "from lynxius.evals.bert_score import BertScore\n",
    "from lynxius.evals.answer_correctness import AnswerCorrectness\n",
    "from lynxius.evals.semantic_similarity import SemanticSimilarity\n",
    "from lynxius.evals.custom_eval import CustomEval\n",
    "from lynxius.evals.context_precision import ContextPrecision\n",
    "from lynxius.evals.json_diff import JsonDiff\n",
    "\n",
    "# ContextChunk represents a document retrieved from you RAG system\n",
    "from lynxius.rag.types import ContextChunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a579bbff-2893-42ca-a92f-3df271a13823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define sample RAG contexts.\n",
    "# Retrieval of context documents depends on the RAG database that you're using\n",
    "context = [\n",
    "    ContextChunk(document=\"Avoid close contact with people who are sick. When you are sick, keep your distance from others to protect them from getting sick, too.\", relevance=0.75),\n",
    "    ContextChunk(document=\"If possible, stay home from work, school, and errands when youâ€™re sick. You can go back to your normal activities when, for at least 24 hours, both are true:\", relevance=0.31)\n",
    "]\n",
    "\n",
    "# Define tags to make it easier to locate these eval runs on the Lynxius platform\n",
    "tags = [\"notebook\", \"experiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "613714a5-e210-46ab-96e7-f84f03d34b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lynxius allows you to use your own evaluator templates\n",
    "# Let's define and use one!\n",
    "\n",
    "# When using a custom template, the only thing that you need to ensure is that\n",
    "# the final verdict is printed at the very bottom of the resonse, with no other characters.\n",
    "custom_eval_template = \"\"\"\n",
    "You are given a question, a reference answer and a candidate answer concerning a clinical matter.\n",
    "You must determine if the candidate answer covers exactly the same content as the reference answer.\n",
    "If the candidate answer contains additional information, or fails to mention something that is present\n",
    "in the reference answer, your verdict should be 'incorrect'. Otherwise, your verdict should be 'correct'.\n",
    "Provide a short explanation about how you arrived to your verdict. The verdict must be printed at the\n",
    "very bottom of your response, on a new line, and it must not contain any extra characters.\n",
    "Here is the data:\n",
    "***********\n",
    "Query: {query}\n",
    "***********\n",
    "Reference answer: {reference}\n",
    "***********\n",
    "Candidate answer: {output}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0abb4ff-8b07-471b-aa76-2d0dca88a815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dac43b97-d359-47f7-be54-4ddcc73cc0b7'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and run the evals\n",
    "bert_score = BertScore(\"PR #111\", level=\"word\", presence_threshold=0.65, tags=tags)\n",
    "answer_correctness = AnswerCorrectness(\"PR #111\", tags=tags)\n",
    "semantic_similarity = SemanticSimilarity(\"PR #111\", tags=tags)\n",
    "custom_eval = CustomEval(\"PR #111\", name=\"clinical_correctness\", prompt_template=custom_eval_template, tags=tags)\n",
    "context_precision = ContextPrecision(\"PR #111\", tags=tags)\n",
    "\n",
    "for entry in dataset_details.entries:\n",
    "    # Query our LLM\n",
    "    actual_output = chatdoctor_v1(entry.query)\n",
    "\n",
    "    # Add traces to the evals\n",
    "    bert_score.add_trace(reference=entry.reference, output=actual_output, context=context)\n",
    "    answer_correctness.add_trace(query=entry.query, reference=entry.reference, output=actual_output, context=context)\n",
    "    semantic_similarity.add_trace(reference=entry.reference, output=actual_output, context=context)\n",
    "    custom_eval.add_trace(values={\"query\": entry.query, \"reference\": entry.reference, \"output\": actual_output}, context=context)\n",
    "    context_precision.add_trace(query=entry.query, reference=entry.reference, context=context)\n",
    "\n",
    "# Run evals remotely\n",
    "client.evaluate(bert_score)\n",
    "client.evaluate(answer_correctness)\n",
    "client.evaluate(semantic_similarity)\n",
    "client.evaluate(custom_eval)\n",
    "client.evaluate(context_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "980f904d-2111-4973-88c4-54c496c50bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f094f7f5-e229-44d9-b5c8-5106b578507f'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and run the JsonDiff eval\n",
    "json_diff = JsonDiff(\"PR #111\", tags=tags)\n",
    "\n",
    "ref = {\n",
    "    \"prop1\": True,\n",
    "    \"prop2\": 0.85,\n",
    "    \"prop3\": [\n",
    "      {\n",
    "        \"prop4\": 0.92,\n",
    "        \"prop5\": 0.71\n",
    "      },\n",
    "      {\n",
    "        \"prop4\": 0.22,\n",
    "        \"prop5\": 1.0\n",
    "      }\n",
    "    ]\n",
    "}\n",
    "output = {\n",
    "    \"prop1\": False,\n",
    "    \"prop2\": 0.71,\n",
    "    \"prop3\": [\n",
    "      {\n",
    "        \"prop4\": 0.89,\n",
    "        \"prop5\": 0.55\n",
    "      },\n",
    "      {\n",
    "        \"prop4\": 0.34,\n",
    "        \"prop5\": 0.97\n",
    "      }\n",
    "    ]\n",
    "}\n",
    "# Weights is an optional parameter. \n",
    "# If not provided, each field will have an equal contribution to the overall score of every nested object.\n",
    "weights = {\n",
    "    \"prop1\": 0.5,\n",
    "    \"prop2\": 0.5,\n",
    "    \"prop3\": 1.0, # Default weights is 1.0 but we can also set it explicitly\n",
    "}\n",
    "\n",
    "json_diff.add_trace(reference=ref, output=output, weights=weights, context=context)\n",
    "client.evaluate(json_diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
