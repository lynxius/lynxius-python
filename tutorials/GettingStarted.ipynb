{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddd87eb4-a4ee-4485-ba10-048127563b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "🔑 Enter your Lynxius API key:  ········\n"
     ]
    }
   ],
   "source": [
    "# First, we have to setup Lynxius API key\n",
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "if not (lynxius_api_key := os.getenv(\"LYNXIUS_API_KEY\")):\n",
    "    lynxius_api_key = getpass(\"🔑 Enter your Lynxius API key: \")\n",
    "\n",
    "os.environ[\"LYNXIUS_API_KEY\"] = lynxius_api_key\n",
    "os.environ[\"LYNXIUS_BASE_URL\"] = \"https://REQUEST-THE-ENDOINT-TO-GET-ACCESS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e2d3e6a-1ad2-47b7-ac34-44961f62e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes it easier to iterate\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfddc746-4f0d-4889-be24-694a6bf618dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lynxius.client import LynxiusClient\n",
    "\n",
    "client = LynxiusClient()\n",
    "\n",
    "# Download the dataset (you can find the ID of your dataset in Lynxius online platform)\n",
    "dataset_details = client.get_dataset_details(dataset_id=\"6e83cec5-d8d3-4237-af9e-8d4b7c71a2ce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "934e9b78-38f4-4674-9817-92fa3e302f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our sample LLM application\n",
    "from datasets_utils import chatdoctor_v1\n",
    "\n",
    "# Importing the evaluators\n",
    "from lynxius.evals.bert_score import BertScore\n",
    "from lynxius.evals.answer_correctness import AnswerCorrectness\n",
    "from lynxius.evals.semantic_similarity import SemanticSimilarity\n",
    "from lynxius.evals.custom_eval import CustomEval\n",
    "from lynxius.evals.context_precision import ContextPrecision\n",
    "\n",
    "# ContextChunk represents a document retrieved from you RAG system\n",
    "from lynxius.rag.types import ContextChunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a579bbff-2893-42ca-a92f-3df271a13823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define sample RAG contexts.\n",
    "# Retrieval of context documents depends on the RAG database that you're using\n",
    "context = [\n",
    "    ContextChunk(document=\"Avoid close contact with people who are sick. When you are sick, keep your distance from others to protect them from getting sick, too.\", relevance=0.75),\n",
    "    ContextChunk(document=\"If possible, stay home from work, school, and errands when you’re sick. You can go back to your normal activities when, for at least 24 hours, both are true:\", relevance=0.31)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "613714a5-e210-46ab-96e7-f84f03d34b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lynxius allows you to use your own evaluator templates\n",
    "# Let's define and use one!\n",
    "\n",
    "# When using a cusotm template, the only thing that you need to ensure is that\n",
    "# the final verdict is printed at the very bottom of the resonse, with no other characters.\n",
    "custom_eval_template = \"\"\"\n",
    "You are given a question, a reference answer and a candidate answer concerning a clinical matter.\n",
    "You must determine if the candidate answer covers exactly the same content as the reference answer.\n",
    "If the candidate answer contains additional information, or fails to mention something that is present\n",
    "in the reference answer, your verdict should be 'incorrect'. Otherwise, your verdict should be 'correct'.\n",
    "Provide a short explanation about how you arrived to your verdict. The verdict must be printed at the\n",
    "very bottom of your response, on a new line, and it must not contain any extra characters.\n",
    "Here is the data:\n",
    "***********\n",
    "Query: {query}\n",
    "***********\n",
    "Reference answer: {reference}\n",
    "***********\n",
    "Candidate answer: {output}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0abb4ff-8b07-471b-aa76-2d0dca88a815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d2c63a48-8f46-4b48-bc9e-d0ce2ef29f5e'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and run the evals\n",
    "bert_score = BertScore(\"PR #111\", level=\"word\", presence_threshold=0.65)\n",
    "answer_correctness = AnswerCorrectness(\"PR #111\")\n",
    "semantic_similarity = SemanticSimilarity(\"PR #111\")\n",
    "custom_eval = CustomEval(\"PR #111\", prompt_template=custom_eval_template)\n",
    "context_precision = ContextPrecision(\"PR #111\")\n",
    "\n",
    "for entry in dataset_details.entries:\n",
    "    # Query our LLM\n",
    "    actual_output = chatdoctor_v1(entry.query)\n",
    "\n",
    "    # Add traces to the evals\n",
    "    bert_score.add_trace(reference=entry.reference, output=actual_output, context=context)\n",
    "    answer_correctness.add_trace(query=entry.query, reference=entry.reference, output=actual_output, context=context)\n",
    "    semantic_similarity.add_trace(reference=entry.reference, output=actual_output, context=context)\n",
    "    custom_eval.add_trace(values={\"query\": entry.query, \"reference\": entry.reference, \"output\": actual_output}, context=context)\n",
    "    context_precision.add_trace(query=entry.query, reference=entry.reference, context=context)\n",
    "\n",
    "# Run!\n",
    "client.evaluate(bert_score)\n",
    "client.evaluate(answer_correctness)\n",
    "client.evaluate(semantic_similarity)\n",
    "client.evaluate(custom_eval)\n",
    "client.evaluate(context_precision)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
