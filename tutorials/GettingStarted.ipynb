{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddd87eb4-a4ee-4485-ba10-048127563b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to setup Lynxius API key\n",
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "if not (lynxius_api_key := os.getenv(\"LYNXIUS_API_KEY\")):\n",
    "    lynxius_api_key = getpass(\"ðŸ”‘ Enter your Lynxius API key: \")\n",
    "\n",
    "os.environ[\"LYNXIUS_API_KEY\"] = lynxius_api_key\n",
    "os.environ[\"LYNXIUS_BASE_URL\"] = \"https://platform.lynxius.ai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e2d3e6a-1ad2-47b7-ac34-44961f62e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes it easier to iterate\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfddc746-4f0d-4889-be24-694a6bf618dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lynxius.client import LynxiusClient\n",
    "\n",
    "client = LynxiusClient()\n",
    "\n",
    "# Download the dataset (you can find the ID of your dataset on Lynxius online platform)\n",
    "dataset_details = client.get_dataset_details(dataset_id=\"7eff0d38-50ee-4b5d-a30d-cf428288016c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "934e9b78-38f4-4674-9817-92fa3e302f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our sample LLM application\n",
    "from datasets_utils import chatdoctor_v1\n",
    "\n",
    "# Importing the evaluators\n",
    "from lynxius.evals.bert_score import BertScore\n",
    "from lynxius.evals.answer_correctness import AnswerCorrectness\n",
    "from lynxius.evals.semantic_similarity import SemanticSimilarity\n",
    "from lynxius.evals.custom_eval import CustomEval\n",
    "from lynxius.evals.context_precision import ContextPrecision\n",
    "from lynxius.evals.json_diff import JsonDiff\n",
    "\n",
    "# ContextChunk represents a document retrieved from you RAG system\n",
    "from lynxius.rag.types import ContextChunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a579bbff-2893-42ca-a92f-3df271a13823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define sample RAG contexts.\n",
    "# Retrieval of context documents depends on the RAG database that you're using\n",
    "context = [\n",
    "    ContextChunk(document=\"Avoid close contact with people who are sick. When you are sick, keep your distance from others to protect them from getting sick, too.\", relevance=0.75),\n",
    "    ContextChunk(document=\"If possible, stay home from work, school, and errands when youâ€™re sick. You can go back to your normal activities when, for at least 24 hours, both are true:\", relevance=0.31)\n",
    "]\n",
    "\n",
    "# Defaine tags to make it easier to locate these eval runs on the Lynxius platform\n",
    "tags = [\"notebook\", \"experiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "613714a5-e210-46ab-96e7-f84f03d34b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lynxius allows you to use your own evaluator templates\n",
    "# Let's define and use one!\n",
    "\n",
    "# When using a cusotm template, the only thing that you need to ensure is that\n",
    "# the final verdict is printed at the very bottom of the resonse, with no other characters.\n",
    "custom_eval_template = \"\"\"\n",
    "You are given a question, a reference answer and a candidate answer concerning a clinical matter.\n",
    "You must determine if the candidate answer covers exactly the same content as the reference answer.\n",
    "If the candidate answer contains additional information, or fails to mention something that is present\n",
    "in the reference answer, your verdict should be 'incorrect'. Otherwise, your verdict should be 'correct'.\n",
    "Provide a short explanation about how you arrived to your verdict. The verdict must be printed at the\n",
    "very bottom of your response, on a new line, and it must not contain any extra characters.\n",
    "Here is the data:\n",
    "***********\n",
    "Query: {query}\n",
    "***********\n",
    "Reference answer: {reference}\n",
    "***********\n",
    "Candidate answer: {output}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0abb4ff-8b07-471b-aa76-2d0dca88a815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'31555855-84f5-4410-b958-c74d674de7c5'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and run the evals\n",
    "bert_score = BertScore(\"PR #111\", level=\"word\", presence_threshold=0.65, tags=tags)\n",
    "answer_correctness = AnswerCorrectness(\"PR #111\", tags=tags)\n",
    "semantic_similarity = SemanticSimilarity(\"PR #111\", tags=tags)\n",
    "custom_eval = CustomEval(\"PR #111\", name=\"clinical_correctness\", prompt_template=custom_eval_template, tags=tags)\n",
    "context_precision = ContextPrecision(\"PR #111\", tags=tags)\n",
    "\n",
    "for entry in dataset_details.entries:\n",
    "    # Query our LLM\n",
    "    actual_output = chatdoctor_v1(entry.query)\n",
    "\n",
    "    # Add traces to the evals\n",
    "    bert_score.add_trace(reference=entry.reference, output=actual_output, context=context)\n",
    "    answer_correctness.add_trace(query=entry.query, reference=entry.reference, output=actual_output, context=context)\n",
    "    semantic_similarity.add_trace(reference=entry.reference, output=actual_output, context=context)\n",
    "    custom_eval.add_trace(values={\"query\": entry.query, \"reference\": entry.reference, \"output\": actual_output}, context=context)\n",
    "    context_precision.add_trace(query=entry.query, reference=entry.reference, context=context)\n",
    "\n",
    "# Run!\n",
    "client.evaluate(bert_score)\n",
    "client.evaluate(answer_correctness)\n",
    "client.evaluate(semantic_similarity)\n",
    "client.evaluate(custom_eval)\n",
    "client.evaluate(context_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "980f904d-2111-4973-88c4-54c496c50bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'72f00957-9511-4694-862c-cc0ee3be4e55'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and run the JsonDiff eval\n",
    "json_diff = JsonDiff(\"PR #111\", tags=tags)\n",
    "\n",
    "ref = {\n",
    "    \"prop1\": True,\n",
    "    \"prop2\": 0.85,\n",
    "    \"prop3\": [\n",
    "      {\n",
    "        \"prop4\": 0.92,\n",
    "        \"prop5\": 0.71\n",
    "      },\n",
    "      {\n",
    "        \"prop4\": 0.22,\n",
    "        \"prop5\": 1.0\n",
    "      }\n",
    "    ]\n",
    "}\n",
    "output = {\n",
    "    \"prop1\": False,\n",
    "    \"prop2\": 0.71,\n",
    "    \"prop3\": [\n",
    "      {\n",
    "        \"prop4\": 0.89,\n",
    "        \"prop5\": 0.55\n",
    "      },\n",
    "      {\n",
    "        \"prop4\": 0.34,\n",
    "        \"prop5\": 0.97\n",
    "      }\n",
    "    ]\n",
    "}\n",
    "# Weights is an optional parameter. \n",
    "# If not provided, each field will have an equal contribution to the overall score of every nested object.\n",
    "weights = {\n",
    "    \"prop1\": 0.5,\n",
    "    \"prop2\": 0.5,\n",
    "    \"prop3\": 1.0, # Default weights is 1.0 but we can also set it explicitly\n",
    "}\n",
    "\n",
    "json_diff.add_trace(reference=ref, output=output, weights=weights, context=context)\n",
    "client.evaluate(json_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5e17ad-9560-4778-b4c2-b3d2d1c0e36a",
   "metadata": {},
   "source": [
    "# Local evaluators\n",
    "Below we demonstrate how to use the same evaluators but run them locally using your own OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca5464cb-4c2c-4ad8-9d80-4b874e5bcff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll be using OpenAI locally so we have to set the API key\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "216da61e-6c65-4ffd-8ac4-16472f1004f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6dceeac6-f90d-4adf-a1c5-e55bfd69d876'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lynxius.evals.local.bert_score import BertScoreLocal\n",
    "from lynxius.evals.local.answer_correctness import AnswerCorrectnessLocal\n",
    "from lynxius.evals.local.context_precision import ContextPrecisionLocal\n",
    "from lynxius.evals.local.semantic_similarity import SemanticSimilarityLocal\n",
    "from lynxius.evals.local.custom_eval import CustomEvalLocal\n",
    "\n",
    "local_tags = [\"local\", \"notebook\", \"experiment\"]\n",
    "\n",
    "bert_score_local = BertScoreLocal(\"PR #222\", level=\"word\", presence_threshold=0.65, tags=local_tags)\n",
    "answer_correctness_local = AnswerCorrectnessLocal(\"PR #222\", tags=local_tags)\n",
    "context_precision_local = ContextPrecisionLocal(\"PR #222\", tags=local_tags)\n",
    "semantic_similarity_local = SemanticSimilarityLocal(\"PR #222\", tags=local_tags)\n",
    "custom_eval_local = CustomEvalLocal(\"PR #222\", name=\"clinical_correctness\", prompt_template=custom_eval_template, tags=local_tags)\n",
    "\n",
    "for entry in dataset_details.entries:\n",
    "    # Query our LLM\n",
    "    actual_output = chatdoctor_v1(entry.query)\n",
    "\n",
    "    # Add traces to the evals\n",
    "    bert_score_local.add_trace(reference=entry.reference, output=actual_output, context=context)\n",
    "    answer_correctness_local.add_trace(query=entry.query, reference=entry.reference, output=actual_output, context=context)\n",
    "    semantic_similarity_local.add_trace(reference=entry.reference, output=actual_output, context=context)\n",
    "    custom_eval_local.add_trace(values={\"query\": entry.query, \"reference\": entry.reference, \"output\": actual_output}, context=context)\n",
    "    context_precision_local.add_trace(query=entry.query, reference=entry.reference, context=context)\n",
    "\n",
    "# Run evals locally and store results in the Lynxius platform\n",
    "client.evaluate(bert_score_local)\n",
    "client.evaluate(answer_correctness_local)\n",
    "client.evaluate(semantic_similarity_local)\n",
    "client.evaluate(custom_eval_local)\n",
    "client.evaluate(context_precision_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c013351-d86c-4d86-8966-f8083abeba86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'78bcc821-6950-4d68-a376-0f1e89ecb469'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lynxius.evals.local.json_diff import JsonDiffLocal\n",
    "\n",
    "json_diff_local = JsonDiffLocal(\"PR #222\", tags=local_tags)\n",
    "json_diff_local.add_trace(reference=ref, output=output, weights=weights, context=context)\n",
    "\n",
    "client.evaluate(json_diff_local)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
