{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatDoctor (LLM App)\n",
    "\n",
    "ChatDoctor is an LLLM chatbot that can assist patients providing medical answers. Let's see how we can test and evaluate this LLM App with [Lynxius](https://www.lynxius.ai/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To set up [Lynxius](https://www.lynxius.ai/) you only need to import the `LYNXIUS_API_KEY` and install [Lynxius](https://www.lynxius.ai/) library with `pip install lynxius`. In this tutorial we are going to run [Lynxius](https://www.lynxius.ai/) directly from it's source code instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔑 Enter your Lynxius API key:  ········\n"
     ]
    }
   ],
   "source": [
    "# First, we have to setup Lynxius API key\n",
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "if not (lynxius_api_key := os.getenv(\"LYNXIUS_API_KEY\")):\n",
    "    lynxius_api_key = getpass(\"🔑 Enter your Lynxius API key: \")\n",
    "\n",
    "os.environ[\"LYNXIUS_API_KEY\"] = lynxius_api_key\n",
    "os.environ[\"LYNXIUS_BASE_URL\"] = \"https://REQUEST-THE-ENDPOINT-TO-GET-ACCESS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run these [Postman](https://www.postman.com/) collections to upload to your platform the datasets used in this notebook:\n",
    "\n",
    "1. [Lynxius ChatDoctor Project (1/3)](./data/postman/Lynxius_ChatDoctor_Project_1_of_3.postman_collection.json) to upload **Dataset_v1**\n",
    "2. [Lynxius ChatDoctor Project (2/3)](./data/postman/Lynxius_ChatDoctor_Project_2_of_3.postman_collection.json) to upload **Dataset_v2**\n",
    "3. [Lynxius ChatDoctor Project (3/3)](./data/postman/Lynxius_ChatDoctor_Project_3_of_3.postman_collection.json) to upload **Dataset_v2-labeled**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's evaluate ChatDoctor_v1 against our Dataset_v1\n",
    "\n",
    "There is already **Dataset_v1** stored in your platform that containes some question and ground-truth answer pairs. Let's download it with the `get_dataset_details()` function.\n",
    "\n",
    "We can now evaluate your **ChatDoctor_v1** LLM application by comparing its outputs to **Dataset_v1** queries with the respective ground-truth reference answers provided in **Dataset_v1**. In this notebook we are going to use `BertScore` and `AnswerCorrectness` metrics for the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I prevent the flu?\n",
      "What are the early signs of diabetes?\n",
      "How do I know if I have a food allergy?\n",
      "What should I do if I get a sunburn?\n"
     ]
    }
   ],
   "source": [
    "from lynxius.client import LynxiusClient\n",
    "\n",
    "client = LynxiusClient()\n",
    "\n",
    "# Downloading Dataset_v1 from Lynxius platform\n",
    "dataset_details = client.get_dataset_details(dataset_id=\"DATASET_V1_UUID\")\n",
    "\n",
    "for entry in dataset_details.entries:\n",
    "    print(entry.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fbcdd27d-a973-4b12-a7dc-9ece1e222cb0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets_utils import chatdoctor_v1\n",
    "\n",
    "from lynxius.evals.bert_score import BertScore\n",
    "from lynxius.evals.answer_correctness import AnswerCorrectness\n",
    "\n",
    "label = \"PR #111\"\n",
    "tags = [\"GPT-4\", \"chatdoctor_v1\", \"Dataset_v1\"]\n",
    "bert_score = BertScore(label=label, tags=tags, level=\"sentence\", presence_threshold=0.65)\n",
    "answer_correctness = AnswerCorrectness(label=label, tags=tags)\n",
    "\n",
    "for entry in dataset_details.entries:\n",
    "    actual_output = chatdoctor_v1(entry.query)\n",
    "    \n",
    "    bert_score.add_trace(reference=entry.reference, output=actual_output)\n",
    "    answer_correctness.add_trace(query=entry.query, reference=entry.reference, output=actual_output)\n",
    "\n",
    "# Run!\n",
    "client.evaluate(bert_score)\n",
    "client.evaluate(answer_correctness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🚀🚀🚀 It looks like the evaluations scored pretty well! **ChatDoctor_v1** can be deployed to production! 🚀🚀🚀\n",
    "\n",
    "<img src=\"https://github-public-assets.s3.us-west-1.amazonaws.com/chatdoctorv1_datasetv1.png\" alt=\"chatdoctorv1_datasetv1\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Monitoring\n",
    "\n",
    "⚠️⚠️⚠️ From the [Lynxius](https://www.lynxius.ai/) you can monitor **ChatDoctor_v1** performance and spot quickly that its performance is decreasing over the weeks. It seems your users are asking queries that your **ChatDoctor_v1** cannot reply with a great level of correctness. ⚠️⚠️⚠️\n",
    "\n",
    "<img src=\"https://github-public-assets.s3.us-west-1.amazonaws.com/chatdoctorv1_monitor.png\" alt=\"chatdoctorv1_monitor.png\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Data Capturing\n",
    "\n",
    "Thankfully [Lynxius](https://www.lynxius.ai/) automatically collects your users' queries and empowers you to efficiently debug edge cases. In this case **Dataset_v2** has been automatically collected.\n",
    "\n",
    "It seems like your users are asking about the **symptoms** related to specific conditions and your chatbot is not able to provide correct answers to these new kind of queries.\n",
    "\n",
    "Your Subject Matter Expert (SMEs), like medical doctors 👩‍⚕️👨‍⚕️, can use [Lynxius](https://www.lynxius.ai/) UI to quickly spot this edge case and they can annotate the new queries with high quality reference data ✅\n",
    "\n",
    "<img src=\"https://github-public-assets.s3.us-west-1.amazonaws.com/datasetv2_spotted_symptoms.png\" alt=\"datasetv2_spotted_symptoms\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's evaluate ChatDoctor_v1 against our Dataset_v2-labeled\n",
    "\n",
    "Once your SMEs correctely annotated your new data, you can evaluate again **ChatDoctor_v1** to see the real performance against **Dataset_v2-labeled**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I prevent the flu?\n",
      "What are the early signs of diabetes?\n",
      "How do I know if I have a food allergy?\n",
      "What should I do if I get a sunburn?\n",
      "What are the symptoms of a migraine headache?\n",
      "What are the symptoms of the common cold?\n",
      "What are the symptoms of a urinary tract infection (UTI)?\n"
     ]
    }
   ],
   "source": [
    "# Downloading Dataset_v2 from Lynxius platform\n",
    "dataset_details = client.get_dataset_details(dataset_id=\"DATASET_V2_LABELED_UUID\")\n",
    "\n",
    "for entry in dataset_details.entries:\n",
    "    print(entry.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7097779c-a2af-4a14-bfd0-9a534db466b3'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets_utils import chatdoctor_v1\n",
    "\n",
    "from lynxius.evals.bert_score import BertScore\n",
    "from lynxius.evals.answer_correctness import AnswerCorrectness\n",
    "\n",
    "label = \"PR #222\"\n",
    "tags = [\"GPT-4\", \"chatdoctor_v1\", \"Dataset_v2-labeled\"]\n",
    "bert_score = BertScore(label=label, tags=tags, level=\"sentence\", presence_threshold=0.65)\n",
    "answer_correctness = AnswerCorrectness(label=label, tags=tags)\n",
    "\n",
    "for entry in dataset_details.entries:\n",
    "    actual_output = chatdoctor_v1(entry.query)\n",
    "    \n",
    "    bert_score.add_trace(reference=entry.reference, output=actual_output)\n",
    "    answer_correctness.add_trace(query=entry.query, reference=entry.reference, output=actual_output)\n",
    "\n",
    "# Run!\n",
    "client.evaluate(bert_score)\n",
    "client.evaluate(answer_correctness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❌❌❌ It looks like the evaluations scores were not acceptable ❌❌❌\n",
    "\n",
    "<img src=\"https://github-public-assets.s3.us-west-1.amazonaws.com/chatdoctorv1_datasetv2labeled.png\" alt=\"chatdoctorv1_datasetv2labeled\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to build ChatDoctor_v2\n",
    "\n",
    "🔧🔨🔩 **ChatDoctor_v1** clearly cannot handle the new edge case and the team needs to work hard on the new**ChatDoctor_v2**. 🔧🔨🔩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's evaluate ChatDoctor_v2 against our Dataset_v2-lebeled\n",
    "\n",
    "Once ***ChatDoctor_v2*** is ready we can test it agains the dataset labeled by your SMEs, **Dataset_v2-lebeled**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I prevent the flu?\n",
      "What are the early signs of diabetes?\n",
      "How do I know if I have a food allergy?\n",
      "What should I do if I get a sunburn?\n",
      "What are the symptoms of a migraine headache?\n",
      "What are the symptoms of the common cold?\n",
      "What are the symptoms of a urinary tract infection (UTI)?\n"
     ]
    }
   ],
   "source": [
    "# Downloading Dataset_v2 from Lynxius platform\n",
    "dataset_details = client.get_dataset_details(dataset_id=\"DATASET_V2_LABELED_UUID\")\n",
    "\n",
    "for entry in dataset_details.entries:\n",
    "    print(entry.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f934d5e0-fe25-4884-a51e-79edffd786d0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets_utils import chatdoctor_v2\n",
    "\n",
    "from lynxius.evals.bert_score import BertScore\n",
    "from lynxius.evals.answer_correctness import AnswerCorrectness\n",
    "\n",
    "label = \"PR #333\"\n",
    "tags = [\"GPT-4\", \"chatdoctor_v2\", \"Dataset_v2-labeled\"]\n",
    "bert_score = BertScore(label=label, tags=tags, level=\"sentence\", presence_threshold=0.65)\n",
    "answer_correctness = AnswerCorrectness(label=label, tags=tags)\n",
    "\n",
    "for entry in dataset_details.entries:\n",
    "    actual_output = chatdoctor_v2(entry.query)\n",
    "\n",
    "    bert_score.add_trace(reference=entry.reference, output=actual_output)\n",
    "    answer_correctness.add_trace(query=entry.query, reference=entry.reference, output=actual_output)\n",
    "\n",
    "# Run!\n",
    "client.evaluate(bert_score)\n",
    "client.evaluate(answer_correctness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🚀🚀🚀 Yess!!! It looks like the evaluations scored well! 🚀🚀🚀\n",
    "\n",
    "<img src=\"https://github-public-assets.s3.us-west-1.amazonaws.com/chatdoctorv2_datasetv2labeled.png\" alt=\"chatdoctorv2_datasetv2labeled\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🚀🚀🚀 We can also see that **ChatDoctor_v2** clearly outperforms **ChatDoctor_v1** 🚀🚀🚀\n",
    "\n",
    "<img src=\"https://github-public-assets.s3.us-west-1.amazonaws.com/chatdoctorv2_monitor.png\" alt=\"chatdoctorv2_monitor\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Considerations\n",
    "\n",
    "[Lynxius](https://www.lynxius.ai/) platform helped the team to evaluate their LLM Apps and decide when they were ready to deploy to production ✅✅✅. It also empowered the team to discover production issues fast ✅✅✅ and collect important end user input queries to further improve their product ✅✅✅"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
