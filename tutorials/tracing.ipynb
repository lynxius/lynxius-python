{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb1c52c-454b-4cf0-a4a2-7a8d6b59b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e0edbf7-1c23-4691-9273-aea4d0ea644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes it easier to iterate\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c58bcd2f-319e-4e2a-97e3-125af048e497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "🔑 Enter your OpenAI API key:  ········\n"
     ]
    }
   ],
   "source": [
    "# We'll be using OpenAI to evaluate locally so we have to set the API key\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"🔑 Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5633222c-e879-40ae-b801-999d1662f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================================\n",
    "# This represents our example LLM application structure\n",
    "# ================================================================================================\n",
    "\n",
    "import time\n",
    "from lynxius.rag.types import ContextChunk\n",
    "from lynxius.evals.context_precision import ContextPrecision\n",
    "from lynxius.evals.bert_score import BertScore\n",
    "from lynxius.tracing.observe import lynxius_observe, lynxius_finalize\n",
    "\n",
    "@lynxius_observe\n",
    "def pipeline(query):\n",
    "    vector = embed(query)\n",
    "    contexts = retrieve(vector, topk=5)\n",
    "\n",
    "    # Evaluate the contexts\n",
    "    lynxius_contexts = [ContextChunk(document=c[\"document\"], relevance=c[\"score\"]) for c in contexts]\n",
    "    context_precision = ContextPrecision(label=\"live_rag_lookup\", tags=[\"live\", \"traced\", \"rag\"])\n",
    "    context_precision.add_trace(query=query, reference=\"-\", context=lynxius_contexts)\n",
    "    context_precision.evaluate_local()\n",
    "\n",
    "    full_response = LLM(\n",
    "        template=\"answer this: {query}, given this: {contexts}\",\n",
    "        data={\"query\": query, \"contexts\": contexts},\n",
    "    )\n",
    "    \n",
    "    response = LLM(\n",
    "        template=\"summarize this: {text}\",\n",
    "        data={\"text\": full_response[\"text\"]},\n",
    "    )\n",
    "\n",
    "    # Evaluate the summarization\n",
    "    bert_score = BertScore(\"live_summarization\", level=\"word\", presence_threshold=0.65, tags=[\"live\", \"traced\", \"summarization\"])\n",
    "    bert_score.add_trace(reference=full_response[\"text\"], output=response[\"text\"])\n",
    "    bert_score.evaluate_local()\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "@lynxius_observe\n",
    "def embed(query):\n",
    "    vector = [0.4, 0.3, 0.8, 0.9, 0.1, 0.2]\n",
    "    time.sleep(1.5)\n",
    "    return vector\n",
    "\n",
    "\n",
    "@lynxius_observe\n",
    "def retrieve(vector, topk):\n",
    "    contexts = rag_db(vector, topk)\n",
    "    reranked = rerank(contexts, topk=2)\n",
    "    time.sleep(0.5)\n",
    "    return reranked\n",
    "\n",
    "\n",
    "@lynxius_observe\n",
    "def rag_db(query, topk):\n",
    "    contexts = [\n",
    "        {\"document\": \"text 1 here\", \"score\": 0.7},\n",
    "        {\"document\": \"text 2 here\", \"score\": 0.8},\n",
    "        {\"document\": \"text 3 here\", \"score\": 0.1},\n",
    "        {\"document\": \"text 4 here\", \"score\": 0.2},\n",
    "        {\"document\": \"text 5 here\", \"score\": 0.9},\n",
    "    ]\n",
    "    time.sleep(0.4)\n",
    "    return contexts\n",
    "\n",
    "\n",
    "@lynxius_observe\n",
    "def rerank(contexts, topk):\n",
    "    time.sleep(1)\n",
    "    return contexts[:topk]\n",
    "\n",
    "\n",
    "@lynxius_observe\n",
    "def LLM(template, data, model=\"gpt-4o\"):\n",
    "    time.sleep(2)\n",
    "    return {\n",
    "        \"text\": \"Answer produced by the LLM\",\n",
    "        \"num_tokens\": 1234,\n",
    "        \"model_version\": \"gpt-4o_v123\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4dd8a39-4c1e-4908-a6e9-d1549a3cd57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cd1bcfdb-93fb-4e30-b61a-63315183a9ab',\n",
       " '0074ea3b-1783-49f6-9cde-a254654debe8']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lynxius.client import LynxiusClient\n",
    "\n",
    "# Run your pipeline with tracing\n",
    "pipeline(\"User query here\")\n",
    "\n",
    "# Get the trace\n",
    "trace = lynxius_finalize()\n",
    "\n",
    "# And store it in a dedicated Lynxius project!\n",
    "client_testing = LynxiusClient(api_key=\"PU7Mf8iDMVcH2ElMaabChQP6zkLqb2cTbrlfnIagGAuHhWyj\", run_local=True)\n",
    "client_testing.store_traces([trace])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
